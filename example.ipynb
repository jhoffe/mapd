{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mapd\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset MNIST\n    Number of datapoints: 60000\n    Root location: data\n    Split: Train"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST_ROOT = \"data\"\n",
    "torchvision.datasets.MNIST(root=MNIST_ROOT, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IDXDataset(Dataset):\n",
    "    def __init__(self, dataset: Dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index], index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "import torch\n",
    "from lightning import LightningModule\n",
    "from typing import Any, Optional, List, Dict\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MAPDModule(LightningModule, metaclass=ABCMeta):\n",
    "    mapd_current_indices_: Optional[torch.Tensor]\n",
    "    mapd_losses_: Tensor\n",
    "    mapd_indices_: Tensor\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mapd_current_indices_ = []\n",
    "        self.mapd_losses_ = torch.empty(0, dtype=torch.float32)\n",
    "        self.mapd_indices_ = torch.empty(0, dtype=torch.int64)\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def batch_loss(self, logits: Any, y: Any) -> Tensor:\n",
    "        raise NotImplemented(\"batch_loss method not implemented\")\n",
    "\n",
    "    def on_before_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:\n",
    "        batch, indices = batch\n",
    "        self.mapd_current_indices_ = indices\n",
    "        return batch\n",
    "\n",
    "    def mapd_log(self, logits: Any, y: Any):\n",
    "        loss = self.batch_loss(logits, y).detach()\n",
    "        current_indices = self.mapd_current_indices_\n",
    "\n",
    "        self.mapd_losses_ = torch.cat((self.mapd_losses_, loss))\n",
    "        self.mapd_indices_ = torch.cat((self.mapd_indices_, current_indices))\n",
    "\n",
    "    def _reset_mapd_attrs(self):\n",
    "        self.mapd_losses_ = torch.empty(0, dtype=torch.float32)\n",
    "        self.mapd_indices_ = torch.empty(0, dtype=torch.int64)\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        #self._reset_mapd_attrs()\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Run loss logging for probes\n",
    "        #dataloader = self.probe_suite_dataloader()\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch\n",
    "\n",
    "\n",
    "class ResNet18(MAPDModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_epochs: int = 10,\n",
    "        lr: float = 0.05,\n",
    "        momentum: float = 0.9,\n",
    "        weight_decay: float = 0.0005\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def batch_loss(self, logits, y) -> torch.Tensor:\n",
    "        return F.cross_entropy(logits, y, reduction=\"none\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        logits = self.forward(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.mapd_log(logits, y)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.lr\n",
    "        )\n",
    "\n",
    "        return {\"optimizer\": optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class MNISTDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"data\", batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "        self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "        mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "        self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "        self.mnist_train = IDXDataset(self.mnist_train)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.mnist_predict, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonashoffmann/miniconda3/envs/mapd3/lib/python3.9/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/jonashoffmann/miniconda3/envs/mapd3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "/home/jonashoffmann/miniconda3/envs/mapd3/lib/python3.9/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | Net  | 21.8 K\n",
      "-------------------------------\n",
      "21.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "21.8 K    Total params\n",
      "0.087     Total estimated model params size (MB)\n",
      "/home/jonashoffmann/miniconda3/envs/mapd3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "579bb34bd18646f59d2f0cc1f6172cf2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonashoffmann/miniconda3/envs/mapd3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:370: UserWarning: You have overridden `on_before_batch_transfer` in `LightningModule` but have passed in a `LightningDataModule`. It will use the implementation from `LightningModule` instance.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "module = ResNet18()\n",
    "dm = MNISTDataModule()\n",
    "\n",
    "trainer = L.Trainer(accelerator=\"cpu\", max_epochs=1)\n",
    "\n",
    "trainer.fit(module, datamodule=dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
