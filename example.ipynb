{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import mapd\n",
    "import torchvision\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset MNIST\n    Number of datapoints: 60000\n    Root location: data\n    Split: Train"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST_ROOT = \"data\"\n",
    "torchvision.datasets.MNIST(root=MNIST_ROOT, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class IDXDataset(Dataset):\n",
    "    def __init__(self, dataset: Dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index], index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "import torch\n",
    "from lightning import LightningModule\n",
    "from typing import Any, Optional, List, Dict\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class MAPDModule(LightningModule, metaclass=ABCMeta):\n",
    "    mapd_current_indices_: torch.Tensor = torch.empty(0, dtype=torch.int64)\n",
    "    mapd_indices_: List[Tensor] = []\n",
    "\n",
    "    mapd_losses_: Tensor = torch.empty(0, dtype=torch.float32)\n",
    "    mapd_proxy_metrics_: List[Tensor] = []\n",
    "\n",
    "    as_proxies_: bool = False\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mapd_current_indices_ = torch.empty(0, dtype=torch.int64)\n",
    "        self.mapd_losses_ = torch.empty(0, dtype=torch.float32)\n",
    "        self.mapd_proxy_metrics_ = []\n",
    "        self.mapd_indices_ = []\n",
    "        self.as_proxies_ = False\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def batch_loss(self, logits: Any, y: Any) -> Tensor:\n",
    "        raise NotImplemented(\"batch_loss method not implemented\")\n",
    "\n",
    "    def on_before_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:\n",
    "        batch, indices = batch\n",
    "        self.mapd_current_indices_ = indices\n",
    "        return batch\n",
    "\n",
    "    def batch_proxy_metric(self, logits: Any, y: Any) -> Tensor:\n",
    "        softmax = torch.softmax(logits, dim=1)\n",
    "\n",
    "        # softmax confidence on correct class\n",
    "        return softmax[torch.arange(softmax.shape[0]), y]\n",
    "\n",
    "    def _mapd_log_proxy(self, logits: Any, y: Any):\n",
    "        proxy_metrics = self.batch_proxy_metric(logits, y).detach()\n",
    "        self.mapd_proxy_metrics_.append(proxy_metrics)\n",
    "\n",
    "    def _mapd_log_probes(self, logits: Any, y: Any):\n",
    "        pass\n",
    "\n",
    "    def mapd_log(self, logits: Any, y: Any) -> \"MAPDModule\":\n",
    "        self.mapd_indices_.append(self.mapd_current_indices_)\n",
    "\n",
    "        if self.as_proxies_:\n",
    "            self._mapd_log_proxy(logits, y)\n",
    "            return self\n",
    "\n",
    "        self._mapd_log_probes(logits, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _reset_mapd_attrs(self) -> \"MAPDModule\":\n",
    "        self.mapd_current_indices_ = torch.empty(0, dtype=torch.int64)\n",
    "        self.mapd_losses_ = torch.empty(0, dtype=torch.float32)\n",
    "        self.mapd_proxy_metrics_ = []\n",
    "        self.mapd_indices_ = []\n",
    "\n",
    "        return self\n",
    "\n",
    "    def as_proxies(self) -> \"MAPDModule\":\n",
    "        self.as_proxies_ = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _write_proxies(self):\n",
    "        sample_indices = torch.cat(self.mapd_indices_).cpu().numpy()\n",
    "        sample_proxy_metrics = torch.cat(self.mapd_proxy_metrics_).numpy()\n",
    "        epochs = np.full(sample_indices.shape, self.current_epoch)\n",
    "\n",
    "        table = pa.table(\n",
    "            [\n",
    "                pa.array(sample_indices),\n",
    "                pa.array(sample_proxy_metrics),\n",
    "                pa.array(epochs),\n",
    "            ],\n",
    "            names=[\"sample_index\", \"proxy_metric\", \"epoch\"],\n",
    "        )\n",
    "\n",
    "        ds.write_dataset(table, \"proxies\",\n",
    "                         partitioning=ds.partitioning(pa.schema([(\"epoch\", pa.int64())]), flavor=\"filename\"),\n",
    "                         existing_data_behavior=\"delete_matching\", format=\"parquet\")\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        self._write_proxies()\n",
    "\n",
    "        self._reset_mapd_attrs()\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Run loss logging for probes\n",
    "        #dataloader = self.probe_suite_dataloader()\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch\n",
    "\n",
    "\n",
    "class ResNet18(MAPDModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            max_epochs: int = 10,\n",
    "            lr: float = 0.05,\n",
    "            momentum: float = 0.9,\n",
    "            weight_decay: float = 0.0005\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "\n",
    "    def mapd_settings(self):\n",
    "        return {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def batch_loss(self, logits, y) -> torch.Tensor:\n",
    "        return F.cross_entropy(logits, y, reduction=\"none\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        logits = self.forward(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        self.mapd_log(logits, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.lr\n",
    "        )\n",
    "\n",
    "        return {\"optimizer\": optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MNISTDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"data\", batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "        self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "        mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "        self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "        self.mnist_train = IDXDataset(self.mnist_train)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.mnist_predict, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "No supported gpu backend found!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mMisconfigurationException\u001B[0m                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[62], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m module \u001B[38;5;241m=\u001B[39m ResNet18()\n\u001B[1;32m      2\u001B[0m dm \u001B[38;5;241m=\u001B[39m MNISTDataModule()\n\u001B[0;32m----> 4\u001B[0m trainer_proxy \u001B[38;5;241m=\u001B[39m \u001B[43mL\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgpu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m trainer_probes \u001B[38;5;241m=\u001B[39m L\u001B[38;5;241m.\u001B[39mTrainer(accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m, max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Proxy\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/mapd3/lib/python3.9/site-packages/lightning/pytorch/utilities/argparse.py:69\u001B[0m, in \u001B[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     66\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mlist\u001B[39m(env_variables\u001B[38;5;241m.\u001B[39mitems()) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlist\u001B[39m(kwargs\u001B[38;5;241m.\u001B[39mitems()))\n\u001B[1;32m     68\u001B[0m \u001B[38;5;66;03m# all args were already moved to kwargs\u001B[39;00m\n\u001B[0;32m---> 69\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/mapd3/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:393\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[0;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001B[0m\n\u001B[1;32m    390\u001B[0m \u001B[38;5;66;03m# init connectors\u001B[39;00m\n\u001B[1;32m    391\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_connector \u001B[38;5;241m=\u001B[39m _DataConnector(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 393\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accelerator_connector \u001B[38;5;241m=\u001B[39m \u001B[43m_AcceleratorConnector\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_nodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_nodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m    \u001B[49m\u001B[43msync_batchnorm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msync_batchnorm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbenchmark\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbenchmark\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    400\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_distributed_sampler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_distributed_sampler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    401\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdeterministic\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprecision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprecision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[43m    \u001B[49m\u001B[43mplugins\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mplugins\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_logger_connector \u001B[38;5;241m=\u001B[39m _LoggerConnector(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback_connector \u001B[38;5;241m=\u001B[39m _CallbackConnector(\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/mapd3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:154\u001B[0m, in \u001B[0;36m_AcceleratorConnector.__init__\u001B[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001B[0m\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accelerator_flag \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_choose_auto_accelerator()\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accelerator_flag \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accelerator_flag \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_choose_gpu_accelerator_backend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_device_config_and_set_final_flags(devices\u001B[38;5;241m=\u001B[39mdevices, num_nodes\u001B[38;5;241m=\u001B[39mnum_nodes)\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_parallel_devices_and_init_accelerator()\n",
      "File \u001B[0;32m~/miniconda3/envs/mapd3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:371\u001B[0m, in \u001B[0;36m_AcceleratorConnector._choose_gpu_accelerator_backend\u001B[0;34m()\u001B[0m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m CUDAAccelerator\u001B[38;5;241m.\u001B[39mis_available():\n\u001B[1;32m    370\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 371\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m MisconfigurationException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo supported gpu backend found!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mMisconfigurationException\u001B[0m: No supported gpu backend found!"
     ]
    }
   ],
   "source": [
    "module = ResNet18()\n",
    "dm = MNISTDataModule()\n",
    "\n",
    "trainer_proxy = L.Trainer(accelerator=\"gpu\", max_epochs=1)\n",
    "trainer_probes = L.Trainer(accelerator=\"cpu\", max_epochs=1)\n",
    "\n",
    "# Proxy\n",
    "trainer_proxy.fit(module.as_proxies(), datamodule=dm)\n",
    "\n",
    "# Probes\n",
    "#trainer_probes.fit(module.as_probes(), datamodule=dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
